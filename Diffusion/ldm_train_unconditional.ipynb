{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0154a-ff8c-4e7b-9313-6c8a1155d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# celelb에서 이미지 읽어오는 Dataloader 만들기\n",
    "# vae latent shape에 대응하도록 모델 짜기\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoencoderKL\n",
    "from model.unet import Unet\n",
    "import os\n",
    "from utils.utils import visualize\n",
    "from utils.sample import ddpm_sample, ddim_sample\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1785d9-dfdc-4edd-a65f-82c7af82fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "unet = Unet(dim=256, mults=[1, 2, 4], channel_scale=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015ec7b-27ec-4e68-b9af-921742a0b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "total_timesteps = 1000\n",
    "beta_0 = 0.0001\n",
    "beta_T = 0.02\n",
    "sampling_steps = 1000\n",
    "\n",
    "output_dir = './logs_ldm'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    os.makedirs(f\"{output_dir}/valid_imgs/\")\n",
    "    os.makedirs(f\"{output_dir}/weights/\")\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "trainer = {\n",
    "    'train_losses': [],\n",
    "    'valid_losses': [],\n",
    "    'valid_images': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8770a1f-38f2-488d-a9d6-3947a89ff340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "class CelebDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, transforms=None, cache_path=\"\"):\n",
    "        super(CelebDataset, self).__init__()\n",
    "        self.transform = transforms\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            self.data_list = torch.load(cache_path)\n",
    "        else:\n",
    "            self.data_list = []\n",
    "            for single in tqdm(os.listdir(data_path)):\n",
    "                single_path = data_path + '/' + single\n",
    "                img = torchvision.io.read_image(single_path)\n",
    "                self.data_list.append(img)\n",
    "            torch.save(self.data_list, cache_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx].float()\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        return item\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "celeb_dataset = CelebDataset(\"./celeb\", transforms=transform, cache_path=\"./cache.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf80a3-e339-43ca-b253-7f6d81ddcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(celeb_dataset))\n",
    "valid_size = len(celeb_dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(celeb_dataset, [train_size, valid_size])\n",
    "\n",
    "celeb_dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "celeb_dataloader_valid = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2b1d6-2e21-408c-b008-a613663c0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps=1000):\n",
    "    betas = torch.linspace(beta_0, beta_T, steps=timesteps+1)\n",
    "    alphas = 1. - betas\n",
    "    alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return betas, alphas, alphas_bar\n",
    "\n",
    "betas, alphas, alphas_bar = linear_beta_schedule(total_timesteps)\n",
    "betas, alphas, alphas_bar = betas.to(device), alphas.to(device), alphas_bar.to(device)\n",
    "\n",
    "def x_t_sample(x_0, timesteps, noise):\n",
    "    return torch.stack([torch.sqrt(alphas_bar[t])*x_0[idx] + torch.sqrt(1-alphas_bar[t])*noise[idx] for idx, t in enumerate(timesteps)])\n",
    "\n",
    "def x_t_1_sample(x_t, timesteps, predicted_noise, z):\n",
    "    moved_mean = torch.stack([x_t[idx] - (1-alphas[t])/(torch.sqrt(1-alphas_bar[t])) * predicted_noise[idx] for idx, t in enumerate(timesteps)])\n",
    "    return torch.stack([1/torch.sqrt(alphas[t]) * moved_mean[idx] + torch.sqrt(betas[t]) * z[idx] for idx, t in enumerate(timesteps)])\n",
    "\n",
    "def write(text):\n",
    "    with open(f'{output_dir}/logs.txt', 'a') as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cca6bb-77df-41a2-a4df-d90af7e98af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_data = next(iter(celeb_dataloader_train))\n",
    "# batch_data = batch_data.to(device)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# vae.eval()\n",
    "# with torch.no_grad():\n",
    "#     z_0 = vae.encode(batch_data[:8])\n",
    "\n",
    "# z_0 = z_0['latent_dist'].mean\n",
    "# print(z_0.shape)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# timesteps = torch.randint(1, 1001, (8,))\n",
    "# timesteps = timesteps.to(device)\n",
    "\n",
    "# predicted_noise = unet(z_0, timesteps)\n",
    "# print(predicted_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b38c87-c287-4210-ab9c-3d007d95f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "write(f\"\\n\\nTraining start : {datetime.today().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "vae.eval()\n",
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    tqdm_bar = tqdm(total=len(celeb_dataloader_train), desc=\"Diffusion Training\")\n",
    "    \n",
    "    for idx, data in enumerate(celeb_dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_0 = data.to(device)\n",
    "        b, c, h, w = x_0.shape\n",
    "        with torch.no_grad():\n",
    "            z_0 = vae.encode(x_0)\n",
    "            z_0 = z_0['latent_dist'].sample() * 0.18215\n",
    "            del x_0\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        timesteps = torch.randint(1, 1001, (b,))\n",
    "        added_noise = torch.randn_like(z_0)\n",
    "        \n",
    "        z_t = x_t_sample(z_0, timesteps, added_noise)\n",
    "        z_t = z_t.to(device)\n",
    "        timesteps = timesteps.to(device)\n",
    "        added_noise = added_noise.to(device)\n",
    "\n",
    "        predicted_noise = unet(z_t, timesteps)\n",
    "        \n",
    "        loss = F.mse_loss(added_noise, predicted_noise)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tqdm_bar.update()\n",
    "        epoch_loss += loss.cpu().detach().item()\n",
    "        if idx%100==99 and epoch>0:\n",
    "            trainer['train_losses'].append(epoch_loss/idx)\n",
    "        \n",
    "    train_text=f'Epoch {epoch} Train loss - {epoch_loss/len(celeb_dataloader_train)}\\n'\n",
    "    write(train_text)\n",
    "    \n",
    "    del loss\n",
    "    del predicted_noise\n",
    "    del added_noise\n",
    "    del timesteps\n",
    "    del z_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unet.eval()\n",
    "    valid_loss = 0\n",
    "    tqdm_bar = tqdm(total=len(celeb_dataloader_valid), desc=\"Diffusion validation\")\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(celeb_dataloader_valid):\n",
    "            x_0 = data.to(device)\n",
    "            b, c, h, w = x_0.shape\n",
    "            with torch.no_grad():\n",
    "                z_0 = vae.encode(x_0)\n",
    "                z_0 = z_0['latent_dist'].sample() * 0.18215\n",
    "                del x_0\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            timesteps = torch.randint(1, 1001, (b,))\n",
    "            added_noise = torch.randn_like(z_0)\n",
    "            \n",
    "            z_t = x_t_sample(z_0, timesteps, added_noise)\n",
    "            z_t = z_t.to(device)\n",
    "            timesteps = timesteps.to(device)\n",
    "            added_noise = added_noise.to(device)\n",
    "    \n",
    "            predicted_noise = unet(z_t, timesteps)\n",
    "            \n",
    "            loss = F.mse_loss(added_noise, predicted_noise)\n",
    "            \n",
    "            valid_loss += loss.cpu().detach().item()\n",
    "            tqdm_bar.update()\n",
    "\n",
    "            if idx==0:\n",
    "                ddim_timesteps = torch.linspace(1000, 10, 100).int().to(device)\n",
    "                # Inference Sampling\n",
    "                x_t = torch.randn_like(z_0).to(device)\n",
    "                for t in ddim_timesteps:\n",
    "                    t = t.repeat(b)\n",
    "                    predict_noise = unet(x_t, t)\n",
    "                    x_t = ddim_sample(x_t, t, predict_noise, alphas_bar[t], alphas_bar[t-10])\n",
    "\n",
    "                predicted_image = vae.decode(x_t)\n",
    "                trainer['valid_images'].append(predicted_image['sample'][:8].cpu().detach()/2+0.5)\n",
    "                del ddim_timesteps\n",
    "                del predicted_image\n",
    "        \n",
    "        del loss\n",
    "        del predicted_noise\n",
    "        del added_noise\n",
    "        del timesteps\n",
    "        del z_t\n",
    "        torch.cuda.empty_cache()\n",
    "        trainer['valid_losses'].append(valid_loss/len(celeb_dataloader_valid))\n",
    "\n",
    "    if valid_loss/len(celeb_dataloader_valid) <= min(trainer['valid_losses']):\n",
    "        torch.save(unet.state_dict(), f'{output_dir}/weights/model_{epoch}.pth')\n",
    "    valid_text=f'Epoch {epoch} Validation loss - {valid_loss/len(celeb_dataloader_valid)}\\n\\n'\n",
    "    write(valid_text)\n",
    "\n",
    "    plt.plot(trainer['train_losses'])\n",
    "    plt.savefig(f'{output_dir}/train_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(trainer['valid_losses'])\n",
    "    plt.savefig(f'{output_dir}/valid_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    visualize(trainer['valid_images'][-1], epoch=epoch, save=True, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac81f05-a28d-4465-9fcb-1b17373bdcea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
